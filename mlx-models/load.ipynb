{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f2399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "# model_name = \"mlx-community/gpt-oss-20b-MXFP4-Q8\"\n",
    "# model_name = \"mlx-community/Qwen3-30B-A3B-4bit\"\n",
    "# model_name = \"lmstudio-community/gemma-3-270m-it-MLX-8bit\"\n",
    "model_name = \"lmstudio-community/Qwen3-4B-MLX-4bit\"\n",
    "# model_name = \"lmstudio-community/Qwen3-30B-A3B-Thinking-2507-MLX-4bit\"\n",
    "\n",
    "model, tokenizer = load(model_name)\n",
    "\n",
    "prompt = \"explain whats the difference between cross-attn and self-attn\"\n",
    "prompt = \"写一个8000字的短篇恐怖故事\"\n",
    "\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens=8192, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are Polaris Alpha, a large language model from an unknown provider.\n",
    "\n",
    "# Formatting Rules:\n",
    "# - Use Markdown for lists, tables, and styling.\n",
    "# - Use ```code fences``` for all code blocks.\n",
    "# - Format file names, paths, and function names with `inline code` backticks.\n",
    "# - **For all mathematical expressions, you must use dollar-sign delimiters. Use $...$ for inline math and $$...$$ for block math. Do not use (...) or [...] delimiters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96f19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    "# import torch\n",
    "# import os\n",
    "# import signal\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import time\n",
    "# from collections import Counter\n",
    "\n",
    "# cpu_count = os.cpu_count()\n",
    "# print(f\"Number of CPU cores in the system: {cpu_count}\")\n",
    "# half_cpu_count = cpu_count // 2\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\n",
    "# torch.set_num_threads(half_cpu_count)\n",
    "\n",
    "# print(f\"PyTorch threads: {torch.get_num_threads()}\")\n",
    "# print(f\"MKL threads: {os.getenv('MKL_NUM_THREADS')}\")\n",
    "# print(f\"OMP threads: {os.getenv('OMP_NUM_THREADS')}\")\n",
    "\n",
    "# # Load the model and tokenizer\n",
    "# NEW_MODEL_ID = \"huihui-ai/Huihui-Qwen3-4B-Thinking-2507-abliterated\"\n",
    "# print(f\"Load Model {NEW_MODEL_ID} ... \")\n",
    "# quant_config_4 = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     llm_int8_enable_fp32_cpu_offload=True,\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     NEW_MODEL_ID, \n",
    "#     device_map=\"balanced\", \n",
    "#     trust_remote_code=True,\n",
    "#     quantization_config=quant_config_4,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "# )\n",
    "# #print(model)\n",
    "# #print(model.config)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL_ID, trust_remote_code=True)\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# messages = []\n",
    "# skip_prompt=True\n",
    "# skip_special_tokens=True\n",
    "# do_sample = True\n",
    "\n",
    "# class CustomTextStreamer(TextStreamer):\n",
    "#     def __init__(self, tokenizer, skip_prompt=True, skip_special_tokens=True):\n",
    "#         super().__init__(tokenizer, skip_prompt=skip_prompt, skip_special_tokens=skip_special_tokens)\n",
    "#         self.generated_text = \"\"\n",
    "#         self.stop_flag = False\n",
    "#         self.init_time = time.time()  # Record initialization time\n",
    "#         self.end_time = None  # To store end time\n",
    "#         self.first_token_time = None  # To store first token generation time\n",
    "#         self.token_count = 0  # To track total tokens\n",
    "\n",
    "#     def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "#         if self.first_token_time is None and text.strip():  # Set first token time on first non-empty text\n",
    "#             self.first_token_time = time.time()\n",
    "#         self.generated_text += text\n",
    "#         # Count tokens in the generated text\n",
    "#         tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "#         self.token_count += len(tokens)\n",
    "#         print(text, end=\"\", flush=True)\n",
    "#         if stream_end:\n",
    "#             self.end_time = time.time()  # Record end time when streaming ends\n",
    "#         if self.stop_flag:\n",
    "#             raise StopIteration\n",
    "\n",
    "#     def stop_generation(self):\n",
    "#         self.stop_flag = True\n",
    "#         self.end_time = time.time()  # Record end time when generation is stopped\n",
    "\n",
    "#     def get_metrics(self):\n",
    "#         \"\"\"Returns initialization time, first token time, first token latency, end time, total time, total tokens, and tokens per second.\"\"\"\n",
    "#         if self.end_time is None:\n",
    "#             self.end_time = time.time()  # Set end time if not already set\n",
    "#         total_time = self.end_time - self.init_time  # Total time from init to end\n",
    "#         tokens_per_second = self.token_count / total_time if total_time > 0 else 0\n",
    "#         first_token_latency = (self.first_token_time - self.init_time) if self.first_token_time is not None else None\n",
    "#         metrics = {\n",
    "#             \"init_time\": self.init_time,\n",
    "#             \"first_token_time\": self.first_token_time,\n",
    "#             \"first_token_latency\": first_token_latency,\n",
    "#             \"end_time\": self.end_time,\n",
    "#             \"total_time\": total_time,  # Total time in seconds\n",
    "#             \"total_tokens\": self.token_count,\n",
    "#             \"tokens_per_second\": tokens_per_second\n",
    "#         }\n",
    "#         return metrics\n",
    "        \n",
    "# def generate_stream(model, tokenizer, messages, skip_prompt, skip_special_tokens, do_sample, max_new_tokens):\n",
    "#     input_ids = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         tokenize=True,\n",
    "#         add_generation_prompt=True,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "#     tokens = input_ids.to(model.device) \n",
    "#     attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "#     streamer = CustomTextStreamer(tokenizer, skip_prompt=skip_prompt, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "#     def signal_handler(sig, frame):\n",
    "#         streamer.stop_generation()\n",
    "#         print(\"\\n[Generation stopped by user with Ctrl+C]\")\n",
    "\n",
    "#     signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "#     generate_kwargs = {}\n",
    "#     if do_sample:\n",
    "#         generate_kwargs = {\n",
    "#               \"do_sample\": do_sample,\n",
    "#               \"max_length\": max_new_tokens,\n",
    "#               \"temperature\": 0.7,\n",
    "#               \"top_k\": 20,\n",
    "#               \"top_p\": 0.8,\n",
    "#               \"repetition_penalty\": 1.2,\n",
    "#               \"no_repeat_ngram_size\": 2\n",
    "#         }\n",
    "#     else:\n",
    "#         generate_kwargs = {\n",
    "#               \"do_sample\": do_sample,\n",
    "#               \"max_length\": max_new_tokens,\n",
    "#               \"repetition_penalty\": 1.2,\n",
    "#               \"no_repeat_ngram_size\": 2\n",
    "#         }\n",
    "  \n",
    "          \n",
    "#     print(\"Response: \", end=\"\", flush=True)\n",
    "#     try:\n",
    "#         generated_ids = model.generate(\n",
    "#             tokens,\n",
    "#             attention_mask=attention_mask,\n",
    "#             #use_cache=False,\n",
    "#             pad_token_id=tokenizer.pad_token_id,\n",
    "#             streamer=streamer,\n",
    "#             **generate_kwargs\n",
    "#         )\n",
    "#         del generated_ids\n",
    "#     except StopIteration:\n",
    "#         print(\"\\n[Stopped by user]\")\n",
    "\n",
    "#     del input_ids, attention_mask\n",
    "#     torch.cuda.empty_cache()\n",
    "#     signal.signal(signal.SIGINT, signal.SIG_DFL)\n",
    "\n",
    "#     return streamer.generated_text, streamer.stop_flag, streamer.get_metrics()\n",
    "\n",
    "# while True:\n",
    "#     print(f\"skip_prompt: {skip_prompt}\")\n",
    "#     print(f\"skip_special_tokens: {skip_special_tokens}\")\n",
    "#     print(f\"do_sample: {do_sample}\")\n",
    "    \n",
    "#     user_input = input(\"User: \").strip()\n",
    "#     if user_input.lower() == \"/exit\":\n",
    "#         print(\"Exiting chat.\")\n",
    "#         break\n",
    "#     if user_input.lower() == \"/clear\":\n",
    "#         messages = []\n",
    "#         print(\"Chat history cleared. Starting a new conversation.\")\n",
    "#         continue\n",
    "#     if user_input.lower() == \"/skip_prompt\":\n",
    "#         skip_prompt = not skip_prompt\n",
    "#         continue\n",
    "#     if user_input.lower() == \"/skip_special_tokens\":\n",
    "#         skip_special_tokens = not skip_special_tokens\n",
    "#         continue\n",
    "#     if user_input.lower() == \"/do_sample\":\n",
    "#         do_sample = not do_sample\n",
    "#         continue\n",
    "#     if not user_input:\n",
    "#         print(\"Input cannot be empty. Please enter something.\")\n",
    "#         continue\n",
    "    \n",
    "\n",
    "#     messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "#     activated_experts = []\n",
    "#     response, stop_flag, metrics = generate_stream(model, tokenizer, messages, skip_prompt, skip_special_tokens, do_sample, 40960)\n",
    "#     print(\"\\n\\nMetrics:\")\n",
    "#     for key, value in metrics.items():\n",
    "#         print(f\"  {key}: {value}\")\n",
    "\n",
    "#     print(\"\", flush=True)\n",
    "#     if stop_flag:\n",
    "#         continue\n",
    "#     messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b34ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlx_lm import load, generate\n",
    "\n",
    "# model, tokenizer = load(\"mlx-community/DeepSeek-OCR-8bit\")\n",
    "\n",
    "# prompt = \"Describe this image.\"\n",
    "\n",
    "# if tokenizer.chat_template is not None:\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     prompt = tokenizer.apply_chat_template(\n",
    "#         messages, add_generation_prompt=True\n",
    "#     )\n",
    "\n",
    "# response = generate(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     prompt=prompt,\n",
    "#     temperature=0.0,\n",
    "#     image=\"image.png\",\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # python -m mlx_vlm.generate --model mlx-community/DeepSeek-OCR-8bit --max-tokens 100 --temperature 0.0 --prompt \"Describe this image.\" --image <path_to_image>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
